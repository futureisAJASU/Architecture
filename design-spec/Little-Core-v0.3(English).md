This document was written to explain, in a single coherent flow, the design intent and structural choices of **Little Core Architecture v0.3**. The goal is to continuously handle always-on mobile workloads (background/UI/system tasks) at minimum power, while also reacting to short bursty work (scrolling, app switching, simple vector/AI ops) without latency. Rather than brute-forcing performance the way traditional high-performance cores do—via complex large schedulers, very wide pipelines, and huge ROBs—this core aims to maximize **control-centric efficiency**. Accordingly, its key philosophy is **simple-but-smart flow control**, **shared units with aggressive power gating**, and **front-end-hint-based pre-wake-up**. Structurally, it places a **shared L2$ (512KB–1MB per cluster)** above a **32KB 4-way L1I**, but instead of simply increasing capacity, it reduces unnecessary tag/data accesses via **way prediction** and **aggressive power gating**, lowering access energy. Fetch is relatively relaxed at **32B/cycle**, while decode is limited to **3-wide**: given ARM instruction density, fetch width doesn’t need to be overly reduced, but increasing decode width quickly worsens power-efficiency, so the choice targets **IPC/W** rather than maximizing IPC itself. A **predecoder** is integrated into fetch to generate meta-tags and classifications of instructions early, so it can pre-wake gated (powered-off) blocks that will be needed later. Branch prediction avoids heavy hybrid predictors and instead uses **light Gshare + light local history table + small BTB**, suppressing predictor power/area; in exchange, the pipeline is intentionally not made overly deep so the mispredict penalty is structurally reduced. After decode, **Thin & Light Renaming** is provided at about **4-wide**, not for aggressive full OoO renaming, but to relax dependencies and secure minimal parallelism while suppressing renaming complexity/energy. The **ROB is 64 entries**, centered on correctness and memory ordering roles—maintaining coherence, guaranteeing commit order, and controlling store-commit timing—while explicitly avoiding the “maximize speculation window at all costs” approach of big cores.

The core’s central differentiator is **Central Dispatcher + Queue Classifier (CD+QC)**. Traditional OoO schedulers require wake-up networks, CAM-based comparisons, broad ready checks, and large fan-out—bad for power/area/clock—and in little-core workloads the benefit is limited versus the cost. So instead of choosing the “best” instruction late in the backend, this design chooses a control-centric structure: classify instructions accurately early, open only the needed paths, and **pre-wake only the needed units** to eliminate execution-time wake-up latency. QC is not a simple router; it combines decoded results with predecode (or fetch-stage) **class hints** to finalize instruction classes (INT0/INT1/LD·ST/FP scalar/Vector·SME2/Shared MUL·DIV, etc.). Based on that final classification, it dispatches to the destination queue while also pre-waking downstream queues and shared units in advance, minimizing just-before-execution wake-up delay.

The **issue policy** is also intentionally differentiated by *not* having a traditional explicit issue scheduler. In the little-core domain, the dominant problem is not “select the optimal instruction,” but “keep things flowing without getting stuck,” and avoiding whole-pipeline stalls from load misses or a few long-latency ops matters more for perceived efficiency. So each queue uses an approximate policy—**Head + N skip**—that checks readiness only within a limited window (e.g., Head + 1–4). If a ready entry exists, it issues immediately. This avoids CAM-heavy complex selection while still mitigating most simple dependencies and single long-latency-induced blockage. It also uses **ALU direct issue**—feeding execution units directly without an intermediate complex scheduler—to reduce latency and simplify control logic.

Execution units split into **Full-stack ALU (INT0)** and **Light ALU (INT1)**: frequent simple ops (add/and, etc.) go through the light path, while branch compare or complex shifts/bit ops go through the full-stack path, reducing waste from keeping complex ALUs always on and minimizing area. **MUL/DIV**, which are less frequently used and power-hungry, are fully shared as a **shared MUL/DIV** across the cluster, kept **OFF by default via full power gating**, and woken using **predecode hints and QC’s finalized class signals**. Rather than waking only after the unit becomes strictly needed, the design predicts likely need early and spins it up briefly to reduce latency, then turns it back off quickly—combining fast response with aggressive sleep. **FP/Vector/SME2** are also bursty rather than always-on for little cores, so instead of large dedicated resources, FP scalar and Vector/SME2 share a structure under a **shared RF**, with aggressive gating in steady-state. To avoid burst bottlenecks, related queues (Shared FP/Vector queue) and the Data Queue can be made relatively generous; in v0.3, considering possible shared-unit queue pressure, the **shared MUL/DIV queue is expanded to 8 entries**, and the **Data Queue** is provisioned like **16+4 (input 16, weight(AI weight ops) 4)** as a burst buffer. The rationale is that (given the assumption that unused blocks can be fully OFF) simply enlarging and gating is more reasonable for power and design complexity than over-partitioning queues into many banks.

The memory subsystem includes an **AGU** and a **16-entry Store Buffer** to reduce cases where stores stall the whole pipeline due to a small SB. Loads are handled via **Load Logic (or a simplified LSQ)** and L1D, while **SB address compare** simplifies store-to-load forwarding/ordering. For concurrent miss handling, **MSHRs** are configured as a base **4 entries**, plus an additional **4-entry gated extension (4+4)** that can be enabled when needed—improving parallel miss capability without driving large area growth—and a clear fill path to L2 is defined. This, too, can be implemented as variable expansion based on predecoder/queue occupancy signals. Additionally, a **direct bypass for AI compute** is provided: because repeated MAC-style AI operations are highly likely to reuse freshly produced results, it can bypass the LD/ST path and route directly into compute units to resume computation immediately. The design note that CD+QC finalizes early class hints (from predecode) into confirmed decode/classification results, and then uses that confirmation as a trigger to pre-wake downstream queues and shared units before execution, is meant to clearly define timing: **(early) explicit predecode + hint generation → (later) confirmation → (confirmation-based) wake-up trigger**. This supports the structural goal of using power only when needed while minimizing execution-time latency.

Overall, this design aims to be not “a core that schedules well,” but “a core whose flow is designed so scheduling is unnecessary.” It removes the traditional OoO issue scheduler, replacing it with central classification plus limited scanning (Head+N) for approximate scheduling, and combines pre-wake-up with strong power gating to maximize IPC/W for always-on little-core tasks while waking shared units only when needed to absorb bursts. ARM Cortex-A5xx-family cores often pursue “small and simple cores” via in-order or limited OoO and static pipelines; philosophically, this design differs by foregrounding control-centric optimizations like **classification + pre-wake-up + fully gated shared units** rather than simply widening. In particular, CD+QC emphasizes **proactive control**—not a reactive “wake after it becomes necessary,” but an early capture of likely need to wake ahead of time—while also serving the allocator that would otherwise be handled by an existing scheduler by assigning work to queues with the appropriate execution units.

This structure is intended to scale beyond the little core: when extending to a middle core, it can increase decoder width **from 3 to 5-6**, expand ROB **from 64 to ~128–192**, deepen queues, and adjust the Head+N range to scale performance while keeping the central classification + pre-wake-up skeleton. In the middle tier, rather than unconditionally adding a full OoO scheduler, it is natural to add scheduler-like functionality only selectively (e.g., limited scheduling for LD/ST or FP/Vector) to avoid power explosion. Toward a big core, one can further enhance branch prediction, enlarge ROB/width, and increase execution width, while still maintaining the overarching “classification–dispatch–shared-unit” structure, making family scaling relatively straightforward. However, this form has not yet been fully validated under modern OoO practice, so further research is needed in that area. From an implementation-roadmap perspective, the current goal is not full ARM ISA execution, but validating control flow, queue pressure, stall conditions, and wake-up timing. So the plan is to first build CD+QC, queue fill/empty behavior, Head+N skip, wake-up FSM, and a minimal ROB commit model in a cycle-level visualization environment like Logisim to identify bottlenecks and timing; after stabilization and verification, migrate parts (ALU/AGU/some queues) into RTL in a hybrid phase, and finally measure quantitative metrics like IPC/W and latency in full RTL toward a final implementation. In summary, this core is not a “small big core,” but a little core reconstructed around power-centric thinking: it maximizes real-world efficiency through a control-centric structure that classifies well, wakes early, and flows simply, aiming to be the last line of defense so middle/big cores do not need to be woken frequently. 

